{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第三章 PyTorch基础：Tensor和Autograd\n",
    "\n",
    "## 3.1 Tensor\n",
    "\n",
    "Tensor，又名张量，读者可能对这个名词似曾相识，因它不仅在PyTorch中出现过，它也是Theano、TensorFlow、\n",
    "Torch和MxNet中重要的数据结构。关于张量的本质不乏深度的剖析，但从工程角度来讲，可简单地认为它就是一个数组，且支持高效的科学计算。它可以是一个数（标量）、一维数组（向量）、二维数组（矩阵）和更高维的数组（高阶数据）。Tensor和Numpy的ndarrays类似，但PyTorch的tensor支持GPU加速。\n",
    "\n",
    "本节将系统讲解tensor的使用，力求面面俱到，但不会涉及每个函数。对于更多函数及其用法，读者可通过在IPython/Notebook中使用函数名加`?`查看帮助文档，或查阅PyTorch官方文档[^1]。\n",
    "\n",
    "[^1]: http://docs.pytorch.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's begin\n",
    "from __future__ import print_function\n",
    "import torch  as t\n",
    "t.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.1.1 基础操作\n",
    "\n",
    "学习过Numpy的读者会对本节内容感到非常熟悉，因tensor的接口有意设计成与Numpy类似，以方便用户使用。但不熟悉Numpy也没关系，本节内容并不要求先掌握Numpy。\n",
    "\n",
    "从接口的角度来讲，对tensor的操作可分为两类：\n",
    "\n",
    "1. `torch.function`，如`torch.save`等。\n",
    "2. 另一类是`tensor.function`，如`tensor.view`等。\n",
    "\n",
    "为方便使用，对tensor的大部分操作同时支持这两类接口，在本书中不做具体区分，如`torch.sum (torch.sum(a, b))`与`tensor.sum (a.sum(b))`功能等价。\n",
    "\n",
    "而从存储的角度来讲，对tensor的操作又可分为两类：\n",
    "\n",
    "1. 不会修改自身的数据，如 `a.add(b)`， 加法的结果会返回一个新的tensor。\n",
    "2. 会修改自身的数据，如 `a.add_(b)`， 加法的结果仍存储在a中，a被修改了。\n",
    "\n",
    "函数名以`_`结尾的都是inplace方式, 即会修改调用者自己的数据，在实际应用中需加以区分。\n",
    "\n",
    "#### 创建Tensor\n",
    "\n",
    "在PyTorch中新建tensor的方法有很多，具体如表3-1所示。\n",
    "\n",
    "表3-1: 常见新建tensor的方法\n",
    "\n",
    "|函数|功能|\n",
    "|:---:|:---:|\n",
    "|Tensor(\\*sizes)|基础构造函数|\n",
    "|ones(\\*sizes)|全1Tensor|\n",
    "|zeros(\\*sizes)|全0Tensor|\n",
    "|eye(\\*sizes)|对角线为1，其他为0|\n",
    "|arange(s,e,step|从s到e，步长为step|\n",
    "|linspace(s,e,steps)|从s到e，均匀切分成steps份|\n",
    "|rand/randn(\\*sizes)|均匀/标准分布|\n",
    "|normal(mean,std)/uniform(from,to)|正态分布/均匀分布|\n",
    "|randperm(m)|随机排列|\n",
    "\n",
    "其中使用`Tensor`函数新建tensor是最复杂多变的方式，它既可以接收一个list，并根据list的数据新建tensor，也能根据指定的形状新建tensor，还能传入其他的tensor，下面举几个例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1326e+12,  4.5890e-41,  1.1326e+12],\n",
       "        [ 4.5890e-41,  4.4842e-44,  0.0000e+00]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 指定tensor的形状\n",
    "a = t.Tensor(2, 3)\n",
    "a # 数值取决于内存空间的状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用list的数据创建tensor\n",
    "b = t.Tensor([[1,2,3],[4,5,6]])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.tolist() # 把tensor转为list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tensor.size()`返回`torch.Size`对象，它是tuple的子类，但其使用方式与tuple略有区别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_size = b.size()\n",
    "b_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.numel() # b中元素总个数，2*3，等价于b.nelement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.1326e+12,  4.5890e-41,  3.9972e-37],\n",
       "         [ 0.0000e+00,  1.5935e+12,  4.5890e-41]]), tensor([ 2.,  3.]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个和b形状一样的tensor\n",
    "c = t.Tensor(b_size)\n",
    "# 创建一个元素为2和3的tensor\n",
    "d = t.Tensor((2, 3))\n",
    "c, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了`tensor.size()`，还可以利用`tensor.shape`直接查看tensor的形状，`tensor.shape`等价于`tensor.size()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.shape??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要注意的是，`t.Tensor(*sizes)`创建tensor时，系统不会马上分配空间，只是会计算剩余的内存是否足够使用，使用到tensor时才会分配，而其它操作都是在创建完tensor之后马上进行空间分配。其它常用的创建tensor的方法举例如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.ones(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.zeros(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.,  3.,  5.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.arange(1, 6, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1.0000,   5.5000,  10.0000])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.linspace(1, 10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2017,  0.3210,  0.7062],\n",
       "        [ 1.4358,  0.0442,  0.3356]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.randn(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3,  2,  1,  4,  0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.randperm(5) # 长度为5的随机排列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.eye(2, 3) # 对角线为1, 不要求行列数一致"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 常用Tensor操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过`tensor.view`方法可以调整tensor的形状，但必须保证调整前后元素总数一致。`view`不会修改自身的数据，返回的新tensor与源tensor共享内存，也即更改其中的一个，另外一个也会跟着改变。在实际应用中可能经常需要添加或减少某一维度，这时候`squeeze`和`unsqueeze`两个函数就派上用场了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.],\n",
       "        [ 3.,  4.,  5.]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 6)\n",
    "a.view(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  1.,  2.,  3.,  4.,  5.])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.],\n",
       "        [ 3.,  4.,  5.]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.view(-1, 3) # 当某一维为-1的时候，会自动计算它的大小\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  1.,  2.],\n",
       "         [ 3.,  4.,  5.]]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.unsqueeze_(0) # 注意形状，在第1维（下标从0开始）上增加“１”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 3])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  1.,  2.]],\n",
       "\n",
       "         [[ 3.,  4.,  5.]]]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.unsqueeze_(-2) # -2表示倒数第二个维度，并且是在这个位置插入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 1, 3])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 1, 2, 3])\n",
      "torch.Size([1, 1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "c = b.view(1, 1, 1, 2, 3)\n",
    "print(c.shape)\n",
    "c.squeeze_(0) # 压缩第0维的“１”\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "c.squeeze_() # 把所有维度为“1”的压缩\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  1.,  2.,  3.,  4.,  5.])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  1.,  2.]],\n",
       "\n",
       "         [[ 3.,  4.,  5.]]]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[   0.,  100.,    2.]],\n",
       "\n",
       "         [[   3.,    4.,    5.]]]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1] = 100\n",
    "b # a修改，b作为view之后的，也会跟着修改"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a b 是同一个对象不同维度方式的表现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`resize`是另一种可用来调整`size`的方法，但与`view`不同，它可以修改tensor的大小。如果新大小超过了原大小，会自动分配新的内存空间，而如果新大小小于原大小，则之前的数据依旧会被保存，看一个例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[   0.,  100.,    2.]],\n",
       "\n",
       "         [[   3.,    4.,    5.]]]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0.,  100.,    2.]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.resize_(1, 3)\n",
    "b\n",
    "# 这里 b 同样也是只显示了 一部分数据，其实其完整的数据还是被保存的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0.,  100.,    2.,    3.,    4.,    5.])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 同样的，ab共享内存，b本质没有变，那么a也没有变了\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  1.0000e+02,  2.0000e+00],\n",
       "        [ 3.0000e+00,  4.0000e+00,  5.0000e+00],\n",
       "        [ 1.5294e-38,  0.0000e+00,  1.5785e+12]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.resize_(3, 3) # 旧的数据依旧保存着，多出的大小会分配新空间\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0.,  100.,    2.,    3.,    4.,    5.])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为b新创建的空间赋予值\n",
    "b[2,:] = t.Tensor([1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0.,  100.,    2.],\n",
       "        [   3.,    4.,    5.],\n",
       "        [   1.,    1.,    1.]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 此时b的值就变化了\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0.,  100.,    2.,    3.,    4.,    5.])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 但是a和b只共享原来的6个数字，b新创建的部分并不影响a\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 对ab共有的部分进行修改，那么ab都会有反应\n",
    "a[0] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 100.,  100.,    2.],\n",
       "        [   3.,    4.,    5.],\n",
       "        [   1.,    1.,    1.]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 索引操作\n",
    "\n",
    "Tensor支持与numpy.ndarray类似的索引操作，语法上也类似，下面通过一些例子，讲解常用的索引操作。如无特殊说明，索引出来的结果与原tensor共享内存，也即修改一个，另一个会跟着修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5449, -0.8211,  1.2699, -0.5329],\n",
       "        [-1.3744, -1.5667, -0.5754, -0.3414],\n",
       "        [-0.6316, -0.5624, -0.2114,  0.7638]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.randn(3, 4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.5449, -0.8211,  1.2699, -0.5329])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0] # 第0行(下标从0开始)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.5449, -1.3744, -0.6316])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:, 0] # 第0列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2699)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0][2] # 第0行第2个元素，等价于a[0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.5329)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0, -1] # 第0行最后一个元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5449, -0.8211,  1.2699, -0.5329],\n",
       "        [-1.3744, -1.5667, -0.5754, -0.3414]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:2] # 前两行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5449, -0.8211],\n",
       "        [-1.3744, -1.5667]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:2, 0:2] # 前两行，第0,1列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "print(a[0:1, :2].shape) # 第0行，前两列 \n",
    "print(a[0, :2].shape) # 注意两者的区别：形状不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  1,  0],\n",
       "        [ 0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a > 1 # 返回一个ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.2699])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[a>1] # 等价于a.masked_select(a>1)\n",
    "# 选择结果与原tensor不共享内存空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5449, -0.8211,  1.2699, -0.5329],\n",
       "        [-1.3744, -1.5667, -0.5754, -0.3414]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[t.LongTensor([0,1])] # 第0行和第1行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其它常用的选择函数如表3-2所示。\n",
    "\n",
    "表3-2常用的选择函数\n",
    "\n",
    "函数|功能|\n",
    ":---:|:---:|\n",
    "index_select(input, dim, index)|在指定维度dim上选取，比如选取某些行、某些列\n",
    "masked_select(input, mask)|例子如上，a[a>0]，使用ByteTensor进行选取\n",
    "non_zero(input)|非0元素的下标\n",
    "gather(input, dim, index)|根据index，在dim维度上选取数据，输出的size与index一样\n",
    "\n",
    "\n",
    "`gather`是一个比较复杂的操作，对一个2维tensor，输出的每个元素如下：\n",
    "\n",
    "```python\n",
    "out[i][j] = input[index[i][j]][j]  # dim=0\n",
    "out[i][j] = input[i][index[i][j]]  # dim=1\n",
    "```\n",
    "三维tensor的`gather`操作同理，下面举几个例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,   1.,   2.,   3.],\n",
       "        [  4.,   5.,   6.,   7.],\n",
       "        [  8.,   9.,  10.,  11.],\n",
       "        [ 12.,  13.,  14.,  15.]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 16).view(4, 4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,   5.,  10.,  15.]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选取对角线的元素\n",
    "index = t.LongTensor([[0,1,2,3]])\n",
    "a.gather(0, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "  3\n",
       "  6\n",
       "  9\n",
       " 12\n",
       "[torch.FloatTensor of size 4x1]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选取反对角线上的元素\n",
    "index = t.LongTensor([[3,2,1,0]]).t()\n",
    "a.gather(1, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 12   9   6   3\n",
       "[torch.FloatTensor of size 1x4]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选取反对角线上的元素，注意与上面的不同\n",
    "index = t.LongTensor([[3,2,1,0]])\n",
    "a.gather(0, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "  0   3\n",
       "  5   6\n",
       " 10   9\n",
       " 15  12\n",
       "[torch.FloatTensor of size 4x2]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选取两个对角线上的元素\n",
    "index = t.LongTensor([[0,1,2,3],[3,2,1,0]]).t()\n",
    "b = a.gather(1, index)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与`gather`相对应的逆操作是`scatter_`，`gather`把数据从input中按index取出，而`scatter_`是把取出的数据再放回去。注意`scatter_`函数是inplace操作。\n",
    "\n",
    "```python\n",
    "out = input.gather(dim, index)\n",
    "-->近似逆操作\n",
    "out = Tensor()\n",
    "out.scatter_(dim, index)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "  0   0   0   3\n",
       "  0   5   6   0\n",
       "  0   9  10   0\n",
       " 12   0   0  15\n",
       "[torch.FloatTensor of size 4x4]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把两个对角线元素放回去到指定位置\n",
    "c = t.zeros(4,4)\n",
    "c.scatter_(1, index, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 高级索引\n",
    "PyTorch在0.2版本中完善了索引操作，目前已经支持绝大多数numpy的高级索引[^10]。高级索引可以看成是普通索引操作的扩展，但是高级索引操作的结果一般不和原始的Tensor共享内存。 \n",
    "[^10]: https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#advanced-indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  0.,   1.,   2.],\n",
       "         [  3.,   4.,   5.],\n",
       "         [  6.,   7.,   8.]],\n",
       "\n",
       "        [[  9.,  10.,  11.],\n",
       "         [ 12.,  13.,  14.],\n",
       "         [ 15.,  16.,  17.]],\n",
       "\n",
       "        [[ 18.,  19.,  20.],\n",
       "         [ 21.,  22.,  23.],\n",
       "         [ 24.,  25.,  26.]]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = t.arange(0,27).view(3,3,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 14.,  24.])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[[1, 2], [1, 2], [2, 0]] # x[1,1,2]和x[2,2,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 19.,  10.,   1.])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[[2, 1, 0], [0], [1]] # x[2,0,1],x[1,0,1],x[0,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  0.,   1.,   2.],\n",
       "         [  3.,   4.,   5.],\n",
       "         [  6.,   7.,   8.]],\n",
       "\n",
       "        [[ 18.,  19.,  20.],\n",
       "         [ 21.,  22.,  23.],\n",
       "         [ 24.,  25.,  26.]]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[[0, 2], ...] # x[0] 和 x[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor类型\n",
    "\n",
    "Tensor有不同的数据类型，如表3-3所示，每种类型分别对应有CPU和GPU版本(HalfTensor除外)。默认的tensor是FloatTensor，可通过`t.set_default_tensor_type` 来修改默认tensor类型(如果默认类型为GPU tensor，则所有操作都将在GPU上进行)。Tensor的类型对分析内存占用很有帮助。例如对于一个size为(1000, 1000, 1000)的FloatTensor，它有`1000*1000*1000=10^9`个元素，每个元素占32bit/8 = 4Byte内存，所以共占大约4GB内存/显存。HalfTensor是专门为GPU版本设计的，同样的元素个数，显存占用只有FloatTensor的一半，所以可以极大缓解GPU显存不足的问题，但由于HalfTensor所能表示的数值大小和精度有限[^2]，所以可能出现溢出等问题。\n",
    "\n",
    "[^2]: https://stackoverflow.com/questions/872544/what-range-of-numbers-can-be-represented-in-a-16-32-and-64-bit-ieee-754-syste\n",
    "\n",
    "表3-3: tensor数据类型\n",
    "\n",
    "数据类型|\tCPU tensor\t|GPU tensor|\n",
    ":---:|:---:|:--:|\n",
    "32-bit 浮点|\ttorch.FloatTensor\t|torch.cuda.FloatTensor\n",
    "64-bit 浮点|\ttorch.DoubleTensor|\ttorch.cuda.DoubleTensor\n",
    "16-bit 半精度浮点|\tN/A\t|torch.cuda.HalfTensor\n",
    "8-bit 无符号整形(0~255)|\ttorch.ByteTensor|\ttorch.cuda.ByteTensor\n",
    "8-bit 有符号整形(-128~127)|\ttorch.CharTensor\t|torch.cuda.CharTensor\n",
    "16-bit 有符号整形  |\ttorch.ShortTensor|\ttorch.cuda.ShortTensor\n",
    "32-bit 有符号整形 \t|torch.IntTensor\t|torch.cuda.IntTensor\n",
    "64-bit 有符号整形  \t|torch.LongTensor\t|torch.cuda.LongTensor\n",
    "\n",
    "各数据类型之间可以互相转换，`type(new_type)`是通用的做法，同时还有`float`、`long`、`half`等快捷方法。CPU tensor与GPU tensor之间的互相转换通过`tensor.cuda`和`tensor.cpu`方法实现。Tensor还有一个`new`方法，用法与`t.Tensor`一样，会调用该tensor对应类型的构造函数，生成与当前tensor类型一致的tensor。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置默认tensor，注意参数是字符串\n",
    "t.set_default_tensor_type('torch.FloatTensor')\n",
    "# 整型的设置时报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1327e+12,  4.5890e-41,  4.0739e-37],\n",
       "        [ 0.0000e+00,  1.9062e-19, -5.6977e-30]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.Tensor(2,3)\n",
    "a # 现在a是IntTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1327e+12,  4.5890e-41,  4.0739e-37],\n",
       "        [ 0.0000e+00,  1.9062e-19, -5.6977e-30]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把a转成FloatTensor，等价于b=a.type(t.FloatTensor)\n",
    "b = a.float() \n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1327e+12,  4.5890e-41,  4.0739e-37],\n",
       "        [ 0.0000e+00,  1.9062e-19, -5.6977e-30]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a.type_as(b)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1326e+12,  4.5890e-41,  4.1396e-37],\n",
       "        [ 0.0000e+00,  4.4842e-44,  0.0000e+00]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = a.new(2,3) # 等价于torch.IntTensor(2,3)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看函数new的源码\n",
    "a.new??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 恢复之前的默认设置\n",
    "t.set_default_tensor_type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 逐元素操作\n",
    "\n",
    "这部分操作会对tensor的每一个元素(point-wise，又名element-wise)进行操作，此类操作的输入与输出形状一致。常用的操作如表3-4所示。\n",
    "\n",
    "表3-4: 常见的逐元素操作\n",
    "\n",
    "|函数|功能|\n",
    "|:--:|:--:|\n",
    "|abs/sqrt/div/exp/fmod/log/pow..|绝对值/平方根/除法/指数/求余/求幂..|\n",
    "|cos/sin/asin/atan2/cosh..|相关三角函数|\n",
    "|ceil/round/floor/trunc| 上取整/四舍五入/下取整/只保留整数部分|\n",
    "|clamp(input, min, max)|超过min和max部分截断|\n",
    "|sigmod/tanh..|激活函数\n",
    "\n",
    "对于很多操作，例如div、mul、pow、fmod等，PyTorch都实现了运算符重载，所以可以直接使用运算符。如`a ** 2` 等价于`torch.pow(a,2)`, `a * 2`等价于`torch.mul(a,2)`。\n",
    "\n",
    "其中`clamp(x, min, max)`的输出满足以下公式：\n",
    "$$\n",
    "y_i =\n",
    "\\begin{cases}\n",
    "min,  & \\text{if  } x_i \\lt min \\\\\n",
    "x_i,  & \\text{if  } min \\le x_i \\le max  \\\\\n",
    "max,  & \\text{if  } x_i \\gt max\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "`clamp`常用在某些需要比较大小的地方，如取一个tensor的每个元素与另一个数的较大值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  0.5403, -0.4161],\n",
       "        [-0.9900, -0.6536,  0.2837]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 6).view(2, 3)\n",
    "t.cos(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.],\n",
       "        [ 0.,  1.,  2.]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a % 3 # 等价于t.fmod(a, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,   1.,   4.],\n",
       "        [  9.,  16.,  25.]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a ** 2 # 等价于t.pow(a, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.],\n",
      "        [ 3.,  4.,  5.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.,  3.,  3.],\n",
       "        [ 3.,  4.,  5.]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取a中的每一个元素与3相比较大的一个 (小于3的截断成3)\n",
    "print(a)\n",
    "t.clamp(a, min=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  归并操作 \n",
    "此类操作会使输出形状小于输入形状，并可以沿着某一维度进行指定操作。如加法`sum`，既可以计算整个tensor的和，也可以计算tensor中每一行或每一列的和。常用的归并操作如表3-5所示。\n",
    "\n",
    "表3-5: 常用归并操作\n",
    "\n",
    "|函数|功能|\n",
    "|:---:|:---:|\n",
    "|mean/sum/median/mode|均值/和/中位数/众数|\n",
    "|norm/dist|范数/距离|\n",
    "|std/var|标准差/方差|\n",
    "|cumsum/cumprod|累加/累乘|\n",
    "\n",
    "以上大多数函数都有一个参数**`dim`**，用来指定这些操作是在哪个维度上执行的。关于dim(对应于Numpy中的axis)的解释众说纷纭，这里提供一个简单的记忆方式：\n",
    "\n",
    "假设输入的形状是(m, n, k)\n",
    "\n",
    "- 如果指定dim=0，输出的形状就是(1, n, k)或者(n, k)\n",
    "- 如果指定dim=1，输出的形状就是(m, 1, k)或者(m, k)\n",
    "- 如果指定dim=2，输出的形状就是(m, n, 1)或者(m, n)\n",
    "\n",
    "size中是否有\"1\"，取决于参数`keepdim`，`keepdim=True`会保留维度`1`。注意，以上只是经验总结，并非所有函数都符合这种形状变化方式，如`cumsum`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.,  2.,  2.]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = t.ones(2, 3)\n",
    "b.sum(dim = 0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.,  2.,  2.])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keepdim=False，不保留维度\"1\"，注意形状\n",
    "b.sum(dim=0, keepdim=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.,  3.])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.],\n",
      "        [ 3.,  4.,  5.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,   1.,   3.],\n",
       "        [  3.,   7.,  12.]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 6).view(2, 3)\n",
    "print(a)\n",
    "a.cumsum(dim=1) # 沿着行累加"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 比较\n",
    "比较函数中有一些是逐元素比较，操作类似于逐元素操作，还有一些则类似于归并操作。常用比较函数如表3-6所示。\n",
    "\n",
    "表3-6: 常用比较函数\n",
    "\n",
    "|函数|功能|\n",
    "|:--:|:--:|\n",
    "|gt/lt/ge/le/eq/ne|大于/小于/大于等于/小于等于/等于/不等|\n",
    "|topk|最大的k个数|\n",
    "|sort|排序|\n",
    "|max/min|比较两个tensor最大最小值|\n",
    "\n",
    "表中第一行的比较操作已经实现了运算符重载，因此可以使用`a>=b`、`a>b`、`a!=b`、`a==b`，其返回结果是一个`ByteTensor`，可用来选取元素。max/min这两个操作比较特殊，以max来说，它有以下三种使用情况：\n",
    "- t.max(tensor)：返回tensor中最大的一个数\n",
    "- t.max(tensor,dim)：指定维上最大的数，返回tensor和下标\n",
    "- t.max(tensor1, tensor2): 比较两个tensor相比较大的元素\n",
    "\n",
    "至于比较一个tensor和一个数，可以使用clamp函数。下面举例说明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,   3.,   6.],\n",
       "        [  9.,  12.,  15.]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.linspace(0, 15, 6).view(2, 3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 15.,  12.,   9.],\n",
       "        [  6.,   3.,   0.]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = t.linspace(15, 0, 6).view(2, 3)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0],\n",
       "        [ 1,  1,  1]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a>b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  9.,  12.,  15.])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[a>b] # a中大于b的元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15.)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.max(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 15.,   6.]), tensor([ 0,  0]))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.max(b, dim=1) \n",
    "# 第一个返回值的15和6分别表示第0行和第1行最大的元素\n",
    "# 第二个返回值的0和0表示上述最大的数是该行第0个元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 15.,  12.,   9.],\n",
       "        [  9.,  12.,  15.]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.max(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 10.,  10.,  10.],\n",
       "        [ 10.,  12.,  15.]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 比较a和10较大的元素\n",
    "t.clamp(a, min=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 线性代数\n",
    "\n",
    "PyTorch的线性函数主要封装了Blas和Lapack，其用法和接口都与之类似。常用的线性代数函数如表3-7所示。\n",
    "\n",
    "表3-7: 常用的线性代数函数\n",
    "\n",
    "|函数|功能|\n",
    "|:---:|:---:|\n",
    "|trace|对角线元素之和(矩阵的迹)|\n",
    "|diag|对角线元素|\n",
    "|triu/tril|矩阵的上三角/下三角，可指定偏移量|\n",
    "|mm/bmm|矩阵乘法，batch的矩阵乘法|\n",
    "|addmm/addbmm/addmv/addr/badbmm..|矩阵运算\n",
    "|t|转置|\n",
    "|dot/cross|内积/外积\n",
    "|inverse|求逆矩阵\n",
    "|svd|奇异值分解\n",
    "\n",
    "具体使用说明请参见官方文档[^3]，需要注意的是，矩阵的转置会导致存储空间不连续，需调用它的`.contiguous`方法将其转为连续。\n",
    "[^3]: http://pytorch.org/docs/torch.html#blas-and-lapack-operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,   3.,   6.],\n",
       "        [  9.,  12.,  15.]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.t()\n",
    "b.is_contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,   9.],\n",
       "        [  3.,  12.],\n",
       "        [  6.,  15.]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Tensor和Numpy\n",
    "\n",
    "Tensor和Numpy数组之间具有很高的相似性，彼此之间的互操作也非常简单高效。需要注意的是，Numpy和Tensor共享内存。由于Numpy历史悠久，支持丰富的操作，所以当遇到Tensor不支持的操作时，可先转成Numpy数组，处理后再转回tensor，其转换开销很小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.]], dtype=float32)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones([2, 3],dtype=np.float32)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = t.from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = t.Tensor(a) # 也可以直接将numpy对象传入Tensor\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1.,  100.,    1.],\n",
       "        [   1.,    1.,    1.]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0, 1]=100\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1.,  100.,    1.],\n",
       "       [   1.,    1.,    1.]], dtype=float32)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = b.numpy() # a, b, c三个对象共享内存\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**： 当numpy的数据类型和Tensor的类型不一样的时候，数据会被复制，不会共享内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.  1.]\n",
      " [ 1.  1.  1.]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "a = np.ones([2, 3])\n",
    "print(a) # 注意和上面的a的区别（dtype不是float32）\n",
    "print(type(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.]])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "b = t.Tensor(a) # FloatTensor(double64或者float64)\n",
    "print(b)\n",
    "print(type(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = t.from_numpy(a) # 注意c的类型（DoubleTensor）\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0, 1] = 100\n",
    "b # b与a不通向内存，所以即使a改变了，b也不变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1.,  100.,    1.],\n",
       "        [   1.,    1.,    1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c # c与a共享内存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "广播法则(broadcast)是科学运算中经常使用的一个技巧，它在快速执行向量化的同时不会占用额外的内存/显存。\n",
    "Numpy的广播法则定义如下：\n",
    "\n",
    "- 让所有输入数组都向其中shape最长的数组看齐，shape中不足的部分通过在前面加1补齐\n",
    "- 两个数组要么在某一个维度的长度一致，要么其中一个为1，否则不能计算 \n",
    "- 当输入数组的某个维度的长度为1时，计算时沿此维度复制扩充成一样的形状\n",
    "\n",
    "PyTorch当前已经支持了自动广播法则，但是笔者还是建议读者通过以下两个函数的组合手动实现广播法则，这样更直观，更不易出错：\n",
    "\n",
    "- `unsqueeze`或者`view`：为数据某一维的形状补1，实现法则1\n",
    "- `expand`或者`expand_as`，重复数组，实现法则3；该操作不会复制数组，所以不会占用额外的空间。\n",
    "\n",
    "注意，repeat实现与expand相类似的功能，但是repeat会把相同数据复制多份，因此会占用额外的空间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = t.ones(3, 2)\n",
    "b = t.zeros(2, 3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.,  1.],\n",
       "         [ 1.,  1.],\n",
       "         [ 1.,  1.]],\n",
       "\n",
       "        [[ 1.,  1.],\n",
       "         [ 1.,  1.],\n",
       "         [ 1.,  1.]]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自动广播法则\n",
    "# 第一步：a是2维,b是3维，所以先在较小的a前面补1 ，\n",
    "#               即：a.unsqueeze(0)，a的形状变成（1，3，2），b的形状是（2，3，1）,\n",
    "# 第二步:   a和b在第一维和第三维形状不一样，其中一个为1 ，\n",
    "#               可以利用广播法则扩展，两个形状都变成了（2，3，2）\n",
    "a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.,  1.],\n",
       "         [ 1.,  1.],\n",
       "         [ 1.,  1.]],\n",
       "\n",
       "        [[ 1.,  1.],\n",
       "         [ 1.,  1.],\n",
       "         [ 1.,  1.]]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 手动广播法则\n",
    "# 或者 a.view(1,3,2).expand(2,3,2)+b.expand(2,3,2)\n",
    "a.unsqueeze(0).expand(2, 3, 2) + b.expand(2,3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# expand不会占用额外空间，只会在需要的时候才扩充，可极大节省内存\n",
    "e = a.unsqueeze(0).expand(10000000000000, 3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000000000000, 3, 2])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 内部结构\n",
    "\n",
    "tensor的数据结构如图3-1所示。tensor分为头信息区(Tensor)和存储区(Storage)，信息区主要保存着tensor的形状（size）、步长（stride）、数据类型（type）等信息，而真正的数据则保存成连续数组。由于数据动辄成千上万，因此信息区元素占用内存较少，主要内存占用则取决于tensor中元素的数目，也即存储区的大小。\n",
    "\n",
    "一般来说一个tensor有着与之相对应的storage, storage是在data之上封装的接口，便于使用，而不同tensor的头信息一般不同，但却可能使用相同的数据。下面看两个例子。\n",
    "\n",
    "![图3-1: Tensor的数据结构](imgs/tensor_data_structure.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.0\n",
       " 1.0\n",
       " 2.0\n",
       " 3.0\n",
       " 4.0\n",
       " 5.0\n",
       "[torch.FloatStorage of size 6]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 6)\n",
    "a.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.0\n",
       " 1.0\n",
       " 2.0\n",
       " 3.0\n",
       " 4.0\n",
       " 5.0\n",
       "[torch.FloatStorage of size 6]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.view(2, 3)\n",
    "b.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 一个对象的id值可以看作它在内存中的地址\n",
    "# storage的内存地址一样，即是同一个storage\n",
    "id(b.storage()) == id(a.storage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0.,  100.,    2.],\n",
       "        [   3.,    4.,    5.]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a改变，b也随之改变，因为他们共享storage\n",
    "a[1] = 100\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.0\n",
       " 100.0\n",
       " 2.0\n",
       " 3.0\n",
       " 4.0\n",
       " 5.0\n",
       "[torch.FloatStorage of size 6]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a[2:] \n",
    "c.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51188056, 51188048)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.data_ptr(), a.data_ptr() # data_ptr返回tensor首元素的内存地址\n",
    "# 可以看出相差8，这是因为2*4=8--相差两个元素，每个元素占4个字节(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0.,  100., -100.,    3.,    4.,    5.])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[0] = -100 # c[0]的内存地址对应a[2]的内存地址\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6666.,   100.,  -100.],\n",
       "        [    3.,     4.,     5.]])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = t.Tensor(c.storage()) # 使用storage创建Tensor，仍然共享内存\n",
    "d[0] = 6666\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 下面４个tensor共享storage\n",
    "id(a.storage()) == id(b.storage()) == id(c.storage()) == id(d.storage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2, 0)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.storage_offset(), c.storage_offset(), d.storage_offset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = b[::2, ::2] # 隔2行/列取一个元素\n",
    "id(e.storage()) == id(a.storage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6666.,   100.,  -100.],\n",
       "        [    3.,     4.,     5.]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 1), (6, 2))"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.stride(), e.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.is_contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可见绝大多数操作并不修改tensor的数据，而只是修改了tensor的头信息。这种做法更节省内存，同时提升了处理速度。在使用中需要注意。\n",
    "此外有些操作会导致tensor不连续，这时需调用`tensor.contiguous`方法将它们变成连续的数据，该方法会使数据复制一份，不再与原来的数据共享storage。\n",
    "另外读者可以思考一下，之前说过的高级索引一般不共享stroage，而普通索引共享storage，这是为什么？（提示：普通索引可以通过只修改tensor的offset，stride和size，而不修改storage来实现）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 其它有关Tensor的话题\n",
    "这部分的内容不好专门划分一小节，但是笔者认为仍值得读者注意，故而将其放在这一小节。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 持久化\n",
    "Tensor的保存和加载十分的简单，使用t.save和t.load即可完成相应的功能。在save/load时可指定使用的`pickle`模块，在load时还可将GPU tensor映射到CPU或其它GPU上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if t.cuda.is_available():\n",
    "    a = a.cuda(1) # 把a转为GPU1上的tensor,\n",
    "    t.save(a,'a.pth')\n",
    "\n",
    "    # 加载为b, 存储于GPU1上(因为保存时tensor就在GPU1上)\n",
    "    b = t.load('a.pth')\n",
    "    # 加载为c, 存储于CPU\n",
    "    c = t.load('a.pth', map_location=lambda storage, loc: storage)\n",
    "    # 加载为d, 存储于GPU0上\n",
    "    d = t.load('a.pth', map_location={'cuda:1':'cuda:0'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####   向量化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "向量化计算是一种特殊的并行计算方式，相对于一般程序在同一时间只执行一个操作的方式，它可在同一时间执行多个操作，通常是对不同的数据执行同样的一个或一批指令，或者说把指令应用于一个数组/向量上。向量化可极大提高科学运算的效率，Python本身是一门高级语言，使用很方便，但这也意味着很多操作很低效，尤其是`for`循环。在科学计算程序中应当极力避免使用Python原生的`for循环`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def for_loop_add(x, y):\n",
    "    result = []\n",
    "    for i,j in zip(x, y):\n",
    "        result.append(i + j)\n",
    "    return t.Tensor(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "547 µs ± 83.4 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "The slowest run took 5.63 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "3.52 µs ± 3 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "x = t.zeros(100)\n",
    "y = t.ones(100)\n",
    "%timeit -n 10 for_loop_add(x, y)\n",
    "%timeit -n 10 x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可见二者有超过40倍的速度差距，因此在实际使用中应尽量调用内建函数(buildin-function)，这些函数底层由C/C++实现，能通过执行底层优化实现高效计算。因此在平时写代码时，就应养成向量化的思维习惯。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外还有以下几点需要注意：\n",
    "- 大多数`t.function`都有一个参数`out`，这时候产生的结果将保存在out指定tensor之中。\n",
    "- `t.set_num_threads`可以设置PyTorch进行CPU多线程并行计算时候所占用的线程数，这个可以用来限制PyTorch所占用的CPU数目。\n",
    "- `t.set_printoptions`可以用来设置打印tensor时的数值精度和格式。\n",
    "下面举例说明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6777e+07) tensor(1.6777e+07)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(2.0000e+05), tensor(2.0000e+05))"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 20000000)\n",
    "print(a[-1], a[-2]) # 32bit的IntTensor精度有限导致溢出\n",
    "b = t.LongTensor()\n",
    "t.arange(0, 200000, out=b) # 64bit的LongTensor不会溢出\n",
    "b[-1],b[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0765, -0.5421, -0.0657],\n",
       "        [ 0.6540, -0.2611, -0.8019]])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.randn(2,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0765383244, -0.5420645475, -0.0657473579],\n",
       "        [0.6539803147, -0.2611482143, -0.8019365072]])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.set_printoptions(precision=10)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5 小试牛刀：线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性回归是机器学习入门知识，应用十分广泛。线性回归利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的，其表达形式为$y = wx+b+e$，$e$为误差服从均值为0的正态分布。首先让我们来确认线性回归的损失函数：\n",
    "$$\n",
    "loss = \\sum_i^N \\frac 1 2 ({y_i-(wx_i+b)})^2\n",
    "$$\n",
    "然后利用随机梯度下降法更新参数$\\textbf{w}$和$\\textbf{b}$来最小化损失函数，最终学得$\\textbf{w}$和$\\textbf{b}$的数值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch as t\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 设置随机数种子，保证在不同电脑上运行时下面的输出一致\n",
    "t.manual_seed(1000) \n",
    "\n",
    "def get_fake_data(batch_size=8):\n",
    "    ''' 产生随机数据：y=x*2+3，加上了一些噪声'''\n",
    "    x = t.rand(batch_size, 1) * 20\n",
    "    y = x * 2 + (1 + t.randn(batch_size, 1))*3\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 来看看产生的x-y分布\n",
    "x, y = get_fake_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1]) torch.Size([8, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fec4fefd470>"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAD+5JREFUeJzt3XFsnPddx/HPB8ej123CrWJK4hRcus3V6Gg8eVFHBOrS\nda4KWr0JISqYgpiUgdjoUGWoh8Q2CVjA2woSUlG2dIlE1a7qPHfqNrKoqZgqQSqnTuOkWSiwruSS\nNq6Y2TpOJXW//HGPS5z5cnf2Pffc/fx+SSff/Z7n+nwfOf348e/53deOCAEAut9PFF0AAKA1CHQA\nSASBDgCJINABIBEEOgAkgkAHgEQQ6ACQCAIdABJBoANAIja082AbN26MwcHBdh4SALrekSNHXoqI\n/nr7tTXQBwcHNTMz085DAkDXs/29RvZjygUAEkGgA0Ai6ga67ctsP2n7adsnbH86G99n+7u2j2aP\nrfmXCwCopZE59Fck7YiIl233SnrC9jezbeMR8XB+5QEAGlU30KPaMP3l7GVv9qCJOgB0mIbm0G33\n2D4q6ZykgxFxONv0F7aP2b7H9k/mViWAQkzPlrV99yFdc/fXtX33IU3PlosuCZfQUKBHxGJEbJW0\nRdI229dLmpB0naR3SbpS0p+s9F7bu2zP2J6Zn59vUdkA8jY9W9bE1JzKCxWFpPJCRRNTc4R6B2tq\nlUtELEh6XNKtEXE2ql6R9CVJ22q8Z09EjETESH9/3XXxADrE5IFTqpxfXDZWOb+oyQOnCqoI9TSy\nyqXfdl/2vCTpFknfsb0pG7OkMUnH8ywUQHudWag0NY7iNbLKZZOk/bZ7VP0B8FBEPGr7kO1+SZZ0\nVNLv5VgngDbb3FdSeYXw3txXKqAaNKKRVS7HJA2vML4jl4oAdITx0SFNTM0tm3Yp9fZofHSowKpw\nKW3t5QKge4wND0iqzqWfWahoc19J46NDr4+j8xDoAGoaGx4gwLsIvVwAIBEEOgAkgkAHgEQQ6ACQ\nCG6KAkBOpmfLbV0lRKADQA6WeuEsreNf6oUjKbdQZ8oFAHJQRC8cAh0AclBELxwCHQByUKvnTZ69\ncAh0AMjB+OiQSr09y8by7oXDTVEAyEERvXAIdADISbt74TDlAgCJINABIBEEOgAkgkAHgERwUxRA\ny7S7dwmWI9ABtEQRvUuwHFMuAFqiiN4lWI5AB9ASRfQuwXIEOoCWKKJ3CZYj0AG0RBG9S7AcN0UB\ntEQRvUuwHIEOoGXa3bsEyzHlAgCJINABIBEEOgAkgkAHgEQQ6ACQCAIdABJBoANAIuoGuu3LbD9p\n+2nbJ2x/Ohu/xvZh2/9m+8u235B/uQCAWhq5Qn9F0o6IuEHSVkm32r5R0l9Juici3iLp+5I+nF+Z\nAIB66gZ6VL2cvezNHiFph6SHs/H9ksZyqRAA0JCG5tBt99g+KumcpIOS/l3SQkS8mu1yWtKKn/e1\nvcv2jO2Z+fn5VtQMAFhBQ4EeEYsRsVXSFknbJF3X6AEiYk9EjETESH9//yrLBADU09Qql4hYkPS4\npHdL6rO91Nxri6Ryi2sDADShkVUu/bb7suclSbdIOqlqsP96tttOSY/kVSQAoL5G2udukrTfdo+q\nPwAeiohHbT8j6UHbfy5pVtLeHOsEANRRN9Aj4pik4RXG/0PV+XQAQAfgk6IAkAgCHQASQaADQCII\ndABIBIEOAIkg0AEgEQQ6ACSCQAeARBDoAJAIAh0AEkGgA0AiCHQASASBDgCJINABIBEEOgAkgkAH\ngEQQ6ACQCAIdABJBoANAIgh0AEgEgQ4AiSDQASARBDoAJIJAB4BEEOgAkAgCHQASQaADQCIIdABI\nBIEOAIkg0AEgEQQ6ACSCQAeARNQNdNtX237c9jO2T9i+Mxv/lO2y7aPZ47b8ywUA1LKhgX1elXRX\nRDxl+82Sjtg+mG27JyI+m195AIBG1Q30iDgr6Wz2/Ie2T0oayLswAEBzmppDtz0oaVjS4Wzoo7aP\n2b7P9hUtrg0A0ISGA932myR9RdLHI+IHku6VdK2krapewX+uxvt22Z6xPTM/P9+CkgEAK2ko0G33\nqhrm90fElCRFxIsRsRgRr0n6gqRtK703IvZExEhEjPT397eqbgDARRpZ5WJJeyWdjIjPXzC+6YLd\nPiDpeOvLAwA0qpFVLtslfUjSnO2j2dgnJN1he6ukkPScpI/kUiEAoCGNrHJ5QpJX2PSN1pcDAFgt\nPikKAIkg0AEgEQQ6ACSCQAeARBDoAJAIAh0AEkGgA0AiCHQASASBDgCJINABIBEEOgAkgkAHgEQQ\n6ACQCAIdABJBoANAIgh0AEgEgQ4AiSDQASARBDoAJIJAB4BEEOgAkAgCHQASQaADQCIIdABIxIai\nC0DapmfLmjxwSmcWKtrcV9L46JDGhgeKLgtIEoGO3EzPljUxNafK+UVJUnmhoompOUki1IEcMOWC\n3EweOPV6mC+pnF/U5IFTBVUEpI1AR27OLFSaGgewNgQ6crO5r9TUOIC1IdCRm/HRIZV6e5aNlXp7\nND46VFBFQNq4KYrcLN34ZJUL0B4EOnI1NjxAgANtwpQLACSibqDbvtr247afsX3C9p3Z+JW2D9p+\nNvt6Rf7lAgBqaeQK/VVJd0XE2yXdKOkPbL9d0t2SHouIt0p6LHsNAChI3UCPiLMR8VT2/IeSTkoa\nkHS7pP3ZbvsljeVVJACgvqbm0G0PShqWdFjSVRFxNtv0gqSrWloZAKApDQe67TdJ+oqkj0fEDy7c\nFhEhKWq8b5ftGdsz8/PzayoWAFBbQ8sWbfeqGub3R8RUNvyi7U0Rcdb2JknnVnpvROyRtEeSRkZG\nVgx9dCc6KQKdpZFVLpa0V9LJiPj8BZu+Jmln9nynpEdaXx461VInxfJCRaH/76Q4PVsuujRg3Wpk\nymW7pA9J2mH7aPa4TdJuSbfYflbSe7PXWCfopAh0nrpTLhHxhCTX2Hxza8tBt6CTItB5+KQoVoVO\nikDnIdDXoenZsrbvPqRr7v66tu8+tKp5bzopAp2H5lzrTKv+LBydFIHOQ6CvM5e6mdlsGNNJEegs\nTLmsM9zMBNJFoK8z3MwE0kWgrzPczATSxRz6OsPNTCBdBPo6xM1MIE1MuQBAIgh0AEgEgQ4AiSDQ\nASARBDoAJIJAB4BEEOgAkAgCHQASQaADQCIIdABIBIEOAIkg0AEgEQQ6ACSCQAeARBDoAJAIAh0A\nEkGgA0AiCHQASASBDgCJ4G+KdqDp2TJ/xBlA0wj0DjM9W9bE1Jwq5xclSeWFiiam5iSJUAdwSUy5\ndJjJA6deD/MllfOLmjxwqqCKAHQLAr3DnFmoNDUOAEsI9A6zua/U1DgALKkb6Lbvs33O9vELxj5l\nu2z7aPa4Ld8y14/x0SGVenuWjZV6ezQ+OlRQRQC6RSNX6Psk3brC+D0RsTV7fKO1Za1fY8MD+swH\n36GBvpIsaaCvpM988B3cEAVQV91VLhHxbduD+ZeCJWPDAwQ4gKatZQ79o7aPZVMyV9TayfYu2zO2\nZ+bn59dwOADApaw20O+VdK2krZLOSvpcrR0jYk9EjETESH9//yoPBwCoZ1WBHhEvRsRiRLwm6QuS\ntrW2LABAs1YV6LY3XfDyA5KO19oXANAedW+K2n5A0k2SNto+LemTkm6yvVVSSHpO0kdyrBEA0IBG\nVrncscLw3hxqAQCsAZ8UBYBEEOgAkAgCHQASQaADQCIIdABIBIEOAIkg0AEgEQQ6ACSCQAeARBDo\nAJAIAh0AEkGgA0AiCHQASASBDgCJINABIBEEOgAkgkAHgEQQ6ACQCAIdABJBoANAIgh0AEgEgQ4A\niSDQASARBDoAJIJAB4BEbCi6gHqmZ8uaPHBKZxYq2txX0vjokMaGB4ouCwA6TkcH+vRsWRNTc6qc\nX5QklRcqmpiakyRCHQAu0tFTLpMHTr0e5ksq5xc1eeBUQRUBQOfq6EA/s1BpahwA1rOODvTNfaWm\nxgFgPevoQB8fHVKpt2fZWKm3R+OjQwVVBACdq6Nvii7d+GSVCwDUVzfQbd8n6dcknYuI67OxKyV9\nWdKgpOck/UZEfD+PAseGBwhwAGhAI1Mu+yTdetHY3ZIei4i3Snosew0AKFDdQI+Ib0v6r4uGb5e0\nP3u+X9JYi+sCADRptTdFr4qIs9nzFyRd1aJ6AACrtOZVLhERkqLWdtu7bM/Ynpmfn1/r4QAANaw2\n0F+0vUmSsq/nau0YEXsiYiQiRvr7+1d5OABAPatdtvg1STsl7c6+PtLIm44cOfKS7R9JemmVx+1E\nG8X5dDLOp7NxPo35uUZ2cnXG5BI72A9IuknVQl+U9ElJ05IekvSzkr6n6rLFi2+c1vrvzUTESCP7\ndgPOp7NxPp2N82mtulfoEXFHjU03t7gWAMAadPRH/wEAjSsi0PcUcMw8cT6djfPpbJxPC9WdQwcA\ndAemXAAgEW0NdNs9tmdtP9rO4+bBdp/th21/x/ZJ2+8uuqa1sP1Htk/YPm77AduXFV1TM2zfZ/uc\n7eMXjF1p+6DtZ7OvVxRZYzNqnM9k9u/tmO2v2u4rssZmrHQ+F2y7y3bY3lhEbatR63xsfyz7Hp2w\n/dftrqvdV+h3SjrZ5mPm5W8l/WNEXCfpBnXxedkekPSHkkayjpo9kn6z2Kqatk9pNZHbpx8/n4OS\nro+IX5T0r5Im2l3UGuzTj5+PbF8t6X2Snm93QWu0Txedj+33qNrn6oaI+AVJn213UW0LdNtbJP2q\npC+265h5sf1Tkn5F0l5Jioj/jYiFYqtasw2SSrY3SLpc0pmC62lKak3kVjqfiPhWRLyavfwXSVva\nXtgq1fj+SNI9kv5Yl2gf0olqnM/vS9odEa9k+9T8BH1e2nmF/jeqfuNea+Mx83KNpHlJX8qmkL5o\n+41FF7VaEVFW9WrieUlnJf13RHyr2KpaIuUmcr8r6ZtFF7EWtm+XVI6Ip4uupUXeJumXbR+2/U+2\n39XuAtoS6LaX/kDGkXYcrw02SHqnpHsjYljSj9Rdv84vk80t367qD6rNkt5o+7eLraq16jWR6ya2\n/1TSq5LuL7qW1bJ9uaRPSPqzomtpoQ2SrpR0o6RxSQ/ZdjsLaNcV+nZJ77f9nKQHJe2w/Q9tOnYe\nTks6HRGHs9cPqxrw3eq9kr4bEfMRcV7SlKRfKrimVmi4iVy3sP07qv4Fsd+K7l5zfK2qFxBPZ7mw\nRdJTtn+m0KrW5rSkqah6UtXZiLbe6G1LoEfERERsiYhBVW+2HYqIrr0CjIgXJP2n7aW/Vn2zpGcK\nLGmtnpd0o+3LsyuKm9XFN3kvsNRETmqiiVynsn2rqtOW74+I/ym6nrWIiLmI+OmIGMxy4bSkd2b/\nb3WraUnvkSTbb5P0BrW58Rjr0FfvY5Lut31M0lZJf1lwPauW/abxsKSnJM2p+u+iqz7BlzWR+2dJ\nQ7ZP2/6wqt1Ab7H9rKq/hewussZm1Difv5P0ZkkHbR+1/feFFtmEGufTtWqcz32Sfj5byvigpJ3t\n/i2KT4oCQCK4QgeARBDoAJAIAh0AEkGgA0AiCHQASASBDgCJINABIBEEOgAk4v8A0ZLGVsvU8YgA\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fec4ffab9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x.squeeze().numpy(), y.squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nH2iNr8LZ1X29AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fec4feab518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8648122549) tensor(3.0800323486)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:42: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    }
   ],
   "source": [
    "# 随机初始化参数\n",
    "w = t.rand(1, 1) \n",
    "b = t.zeros(1, 1)\n",
    "\n",
    "lr =0.001 # 学习率\n",
    "\n",
    "for ii in range(20000):\n",
    "    x, y = get_fake_data()\n",
    "    \n",
    "    # forward：计算loss\n",
    "    y_pred = x.mm(w) + b.expand_as(y) # x@W等价于x.mm(w);for python3 only\n",
    "    loss = 0.5 * (y_pred - y) ** 2 # 均方误差\n",
    "    loss = loss.sum()\n",
    "    \n",
    "    # backward：手动计算梯度\n",
    "    dloss = 1\n",
    "    dy_pred = dloss * (y_pred - y)\n",
    "    \n",
    "    dw = x.t().mm(dy_pred)\n",
    "    db = dy_pred.sum()\n",
    "    \n",
    "    # 更新参数\n",
    "    w.sub_(lr * dw)\n",
    "    b.sub_(lr * db)\n",
    "    \n",
    "    if ii%1000 ==0:\n",
    "       \n",
    "        # 画图\n",
    "        display.clear_output(wait=True)\n",
    "        x = t.arange(0, 20).view(-1, 1)\n",
    "        y = x.mm(w) + b.expand_as(x)\n",
    "        plt.plot(x.numpy(), y.numpy()) # predicted\n",
    "        \n",
    "        x2, y2 = get_fake_data(batch_size=20) \n",
    "        plt.scatter(x2.numpy(), y2.numpy()) # true data\n",
    "        \n",
    "        plt.xlim(0, 20)\n",
    "        plt.ylim(0, 41)\n",
    "        plt.show()\n",
    "        plt.pause(0.5)\n",
    "        \n",
    "print(w.squeeze()[0], b.squeeze()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可见程序已经基本学出w=2、b=3，并且图中直线和数据已经实现较好的拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然上面提到了许多操作，但是只要掌握了这个例子基本上就可以了，其他的知识，读者日后遇到的时候，可以再看看这部份的内容或者查找对应文档。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
